{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o Audio Podcast and Story Example\n",
    "\n",
    "GPT-4o (\"o\" for \"omni\") and GPT-4o mini are multimodal models designed to handle a combination of text, audio, and video inputs, and can generate outputs in text, audio, and image formats. GPT-4o mini is the lightweight version of GPT-4o.\n",
    "\n",
    "Today we are going to use the `gpt-4o-audio-preview` model to generate an expressive podcast and a story.\n",
    "\n",
    "We'll also showcase how to use the structured outputs SDK feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install OpenAI SDK for Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the OpenAI client and submit a test request\n",
    "To setup the client for our use, we need to create an API key to use with our request. Skip these steps if you already have an API key for usage. \n",
    "\n",
    "You can get an API key by following these steps:\n",
    "1. [Create a new project](https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects)\n",
    "2. [Generate an API key in your project](https://platform.openai.com/api-keys)\n",
    "3. (RECOMMENDED, BUT NOT REQUIRED) [Setup your API key for all projects as an env var](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key)\n",
    "\n",
    "Once you have the API key setup, let's move on to setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "import os\n",
    "import base64\n",
    "from datetime import datetime\n",
    "# For structured outputs later\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "## Set the API key\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as an env var>\"))\n",
    "\n",
    "def get_openai_client(api_key=None):\n",
    "    if not api_key:\n",
    "        api_key = os.environ.get(\"OPENAI_API_KEY\", \"<Your OpenAI API key if not set as an env var>\")\n",
    "    return OpenAI(api_key=api_key)\n",
    "\n",
    "## Set the output directory for later\n",
    "output_dir = \"output_audio\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First hello world example output\n",
    "Now let's try exporting an hello world example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_content = \"Hello world!!\"\n",
    "voice = \"echo\" # https://platform.openai.com/docs/guides/text-to-speech#voice-options\n",
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-audio-preview\",  #gpt-4o-audio-preview-2024-12-17\n",
    "        modalities=[\"text\", \"audio\"],\n",
    "        audio={\"voice\": voice, \"format\": \"mp3\"}, #wav\n",
    "        max_tokens = 100,\n",
    "        temperature= 0.2,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a helpful assistant. Say the text exactly as provided\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": speech_content\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error generating audio\")\n",
    "\n",
    "# Decode audio and save to file\n",
    "try:\n",
    "    mp3_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "    speech_file_path = os.path.join(output_dir, f\"helloworld_{timestamp}.mp3\")\n",
    "    with open(speech_file_path, \"wb\") as f:\n",
    "        f.write(mp3_bytes)\n",
    "    print(f\"Saved audio to {speech_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Emotion and Accents\n",
    "GPT-4o is capable of a lot more than that, let's add an accent and emotion parametres into the system message.\n",
    "\n",
    "Make sure to try changing them to see how it impacts the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_content = \"Hello world! Pleased to be here today!\"\n",
    "voice = \"echo\" # https://platform.openai.com/docs/guides/text-to-speech#voice-options\n",
    "accent = \"posh british male\"\n",
    "emotion = \"excited\" #sad  \n",
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-audio-preview\",  #gpt-4o-audio-preview-2024-12-17\n",
    "        modalities=[\"text\", \"audio\"],\n",
    "        audio={\"voice\": voice, \"format\": \"mp3\"}, #wav\n",
    "        temperature= 0.2,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a helpful assistant. Output the text provided, using a {accent} accent and act {emotion}.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": speech_content\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error generating audio\")\n",
    "\n",
    "# Decode audio and save to file\n",
    "try:\n",
    "    mp3_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "    speech_file_path = os.path.join(output_dir, f\"helloworld_{timestamp}.mp3\")\n",
    "    with open(speech_file_path, \"wb\") as f:\n",
    "        f.write(mp3_bytes)\n",
    "    print(f\"Saved audio to {speech_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting this together into a podcast or story \n",
    "1. Define speakers we want and add the content\n",
    "2. Generate a script using GPT-4o with structured outputs\n",
    "2. Process the script and feed to GPT-4o to export the speech\n",
    "3. Assemble the audio outouts into a single audio file, top and tail with intro/outro idents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many as you want, four used here\n",
    "predefined_speakers = [\n",
    "    {\"speaker\": \"Dave\", \"personality\": \"News anchor host, comedian\",\"accent\": \"london british male\", \"voice\": \"ash\"},\n",
    "    {\"speaker\": \"Kelly\",  \"personality\": \"high energy comedian\",\"accent\": \"scottish female radio voice\", \"voice\": \"nova\"},\n",
    "    {\"speaker\": \"Charlie\", \"personality\": \"serious and to the point academic\",\"accent\": \"sassy american female radio voice\", \"voice\": \"sage\"},\n",
    "    {\"speaker\": \"Mike\", \"personality\": \"silly and funny\",\"accent\": \"american male deep raspy\", \"voice\": \"echo\"}\n",
    "]\n",
    "# Add some content - you could load a news article, a PDF or a wikipedia page\n",
    "textinput = \"\"\"\n",
    "The Stargate Project is a new company which intends to invest $500 billion over the next four years building new AI infrastructure for OpenAI in the United States. We will begin deploying $100 billion immediately. This infrastructure will secure American leadership in AI, create hundreds of thousands of American jobs, and generate massive economic benefit for the entire world. This project will not only support the re-industrialization of the United States but also provide a strategic capability to protect the national security of America and its allies.\n",
    "\n",
    "The initial equity funders in Stargate are SoftBank, OpenAI, Oracle, and MGX. SoftBank and OpenAI are the lead partners for Stargate, with SoftBank having financial responsibility and OpenAI having operational responsibility. Masayoshi Son will be the chairman.\n",
    "\n",
    "Arm, Microsoft, NVIDIA, Oracle, and OpenAI are the key initial technology partners. The buildout is currently underway, starting in Texas, and we are evaluating potential sites across the country for more campuses as we finalize definitive agreements.\n",
    "\n",
    "As part of Stargate, Oracle, NVIDIA, and OpenAI will closely collaborate to build and operate this computing system. This builds on a deep collaboration between OpenAI and NVIDIA going back to 2016 and a newer partnership between OpenAI and Oracle.\n",
    "\n",
    "This also builds on the existing OpenAI partnership with Microsoft. OpenAI will continue to increase its consumption of Azure as OpenAI continues its work with Microsoft with this additional compute to train leading models and deliver great products and services.\n",
    "\n",
    "All of us look forward to continuing to build and develop AI—and in particular AGI—for the benefit of all of humanity. We believe that this new step is critical on the path, and will enable creative people to figure out how to use AI to elevate humanity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the script\n",
    "Now taking the article we generate a script using GPT-4o and the unstructured outputs SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for our speakers\n",
    "class ScriptSegment(BaseModel):\n",
    "    speaker: str\n",
    "    personality: str\n",
    "    accent: str\n",
    "    voice: str\n",
    "    content: str \n",
    "\n",
    "class ScriptOutput(BaseModel):\n",
    "    segments: List[ScriptSegment]\n",
    "\n",
    "# Convert predefined speakers to JSON string\n",
    "speakers_json = json.dumps(predefined_speakers, indent=2)\n",
    "\n",
    "# Prepare system prompt with predefined speakers\n",
    "system_message = (\n",
    "    f\"\"\"\n",
    "    Generate a short news podcast script for \"OpenAI News\" using predefined speakers, each with a specific accent and voice. Ensure the script is engaging and fun, incorporating humor where appropriate.\n",
    "    Keep the podcast engaging and short.\n",
    "\n",
    "    - Start with welcoming the listener to \"OpenAI News.\"\n",
    "    - Use realistic, conversational language that is high-energy and humorous if suitable.\n",
    "    - Include interactions between speakers like jokes and laughing, to engage the audience.\n",
    "    - Use the speaker names in the conversation such as \"Over to you Mike\", \"what do you think kelly?\"\n",
    "    - Use short, concise sentences for clarity. maximum 1-3 sentences. \n",
    "\n",
    "    # Script expected output\n",
    "\n",
    "    - **Speaker**: The character or predefined speaker delivering the lines.\n",
    "    - **Personality** The predefined personality of the voice\n",
    "    - **Accent**: The predefined accent in which the speaker should deliver their lines.\n",
    "    - **Voice**: the predefined name of the voice\n",
    "    - **Content**: The dialogue or script that the speaker will say.\n",
    "\n",
    "    Predefined Speakers:\\n{speakers_json}\n",
    "\n",
    "    # Steps\n",
    "\n",
    "    1. Parse the predefined speakers list to understand their accents.\n",
    "    2. For each news segment, assign lines to an appropriate speaker.\n",
    "    3. Ensure each line is engaging and humorous if appropriate, using short, concise language and conversational style.\n",
    "    4. Use tags such as [excitedly], [whispering], [loudly] to indicate how the lines should be said rather than writing \"spoke in blah blah in x accent\"\n",
    "    \n",
    "    # Notes\n",
    "    - Ensure the script encapsulates only enough content for a 2-minute episode. \n",
    "    - Format the JSON meticulously to avoid errors in interpretation. \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Combine messages for the chat\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": textinput},\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 5000,\n",
    "\n",
    "}\n",
    "# Structured outputs\n",
    "script = client.beta.chat.completions.parse(**params, response_format=ScriptOutput)\n",
    "\n",
    "#  Parse the JSON content\n",
    "script_data = script.choices[0].message.parsed.segments\n",
    "\n",
    "# Print the first segment as an example\n",
    "if script_data:\n",
    "    first_segment = script_data[0]\n",
    "    print(\"First Segment of the Podcast Script:\")\n",
    "    print(first_segment.model_dump_json(indent=2))\n",
    "else:\n",
    "    print(\"No script segments found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the audio segments\n",
    "Now let's generate the audio segments and stitch it together.\n",
    "\n",
    "if you don't have pydub installed, let's install that now.\n",
    "\n",
    "If using newer versions of python 3.11 onwards we need to add support for audioop which was removed in 3.13\n",
    "\n",
    "(We make no guarantees about the usability or security of 3rd party software such as PyDub.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pydub audioop-lts\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "audio_segments = []\n",
    "intro_path = \"intro.mp3\"\n",
    "outro_path = \"outro.mp3\"\n",
    "\n",
    "# For each script segment process the audio\n",
    "for idx, segment in enumerate(script_data):\n",
    "    accent = segment.accent\n",
    "    personality = segment.personality\n",
    "    speaker = segment.speaker\n",
    "    voice = segment.voice\n",
    "    speech_content = segment.content\n",
    "\n",
    "    print(f\"Processing segment {idx+1}: {speaker} - {accent}\")\n",
    "\n",
    "    # Currently can only change the voice on each completion request not message.\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini-audio-preview\",  #for better quality use gpt-4o-audio-preview-2024-12-17\n",
    "            modalities=[\"text\", \"audio\"],\n",
    "            max_tokens = 800,\n",
    "            audio={\"voice\": voice, \"format\": \"mp3\"},\n",
    "            temperature= 0.8,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"\"\"You are a helpful assistant. output the speech provided using a {accent} accent and {personality} personality. Never say the type of accent or personality\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": speech_content\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating audio for segment {idx+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Decode audio and save to file\n",
    "    try:\n",
    "        mp3_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        speech_file_path = os.path.join(output_dir, f\"speech_{timestamp}.mp3\")\n",
    "        with open(speech_file_path, \"wb\") as f:\n",
    "            f.write(mp3_bytes)\n",
    "        print(f\"Saved audio to {speech_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving audio for segment {idx+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Load the audio segment using pydub, normalize, and store\n",
    "    try:\n",
    "        audio = AudioSegment.from_mp3(speech_file_path)\n",
    "        audio = normalize(audio)\n",
    "        audio_segments.append(audio)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio for segment {idx+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "try:\n",
    "    if os.path.exists(intro_path):\n",
    "        intro = AudioSegment.from_mp3(intro_path)\n",
    "        intro = normalize(intro)\n",
    "        print(\"Intro music added.\")\n",
    "    else:\n",
    "        intro = AudioSegment.silent(duration=500)\n",
    "        print(\"Intro music not found. Using 0.5 second of silence instead.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding intro music: {e}\")\n",
    "    intro = AudioSegment.silent(duration=1000)\n",
    "\n",
    "try:\n",
    "    if os.path.exists(outro_path):\n",
    "        outro = AudioSegment.from_mp3(outro_path)\n",
    "        outro = normalize(outro)\n",
    "        print(\"Outro music added.\")\n",
    "    else:\n",
    "        outro = AudioSegment.silent(duration=1000)\n",
    "        print(\"Outro music not found. Using 1 second of silence instead.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding outro music: {e}\")\n",
    "    outro = AudioSegment.silent(duration=1000)\n",
    "\n",
    "# Combine all audio: Intro + speech segments + Outro\n",
    "final_podcast = intro\n",
    "for audio in audio_segments:\n",
    "    final_podcast += AudioSegment.silent(duration=250)\n",
    "    final_podcast += audio\n",
    "final_podcast += outro\n",
    "\n",
    "# Export the final podcast\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "final_output_path = f\"podcast_{timestamp}.mp3\"\n",
    "try:\n",
    "    final_podcast.export(final_output_path, format=\"mp3\")\n",
    "    print(f\"Final output saved to {final_output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting final output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The voices seem random?\n",
    "You'll notice that on each segment the voices can change, as of today it's not possible to keep the audio generation consistent and deterministic. \n",
    "\n",
    "It's possible to generate all the segments in one go with a cut segment identifier, then re-slice. We'll show that example soon. \n",
    "\n",
    "Or alternatively you could use a TTS engine at the cost of expressiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about a scary bedtime story?\n",
    "Now let's try making a scary bedtime story, same again but with different intro/outro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predefined_speakers = [\n",
    "    {\"speaker\": \"Zombie Kelly\", \"personality\": \"scary zombie with sass\",\"accent\": \"scottish female\", \"voice\": \"nova\"},\n",
    "    {\"speaker\": \"Scary Harry\", \"personality\": \"scary silly ghost\",\"accent\": \"scottish male raspy\", \"voice\": \"echo\"},\n",
    "]\n",
    "\n",
    "# the story prompt\n",
    "textinput = \"\"\"Peter and Jane go to the haunted castle in Scotland to find the treasure\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scary intros\n",
    "intro_path = \"intro2.mp3\"\n",
    "outro_path = \"outro2.mp3\"\n",
    "\n",
    "# Define a class for our speakers\n",
    "class ScriptSegment(BaseModel):\n",
    "    speaker: str\n",
    "    personality: str\n",
    "    accent: str\n",
    "    voice: str\n",
    "    content: str \n",
    "\n",
    "class ScriptOutput(BaseModel):\n",
    "    segments: List[ScriptSegment]\n",
    "\n",
    "# Convert predefined speakers to JSON string\n",
    "speakers_json = json.dumps(predefined_speakers, indent=2)\n",
    "\n",
    "# Prepare system prompt with predefined speakers\n",
    "system_message = (\n",
    "    f\"\"\"\n",
    "    Generate a short story for a kids radio show using predefined speakers, each with a specific accent and voice. Ensure the script is engaging and fun, incorporating humor where appropriate.\n",
    "    Keep the story engaging and short and suitable for children.\n",
    "\n",
    "    - Start by going straight into the story and set the scene\n",
    "    - Introduce new characters\n",
    "    - Use short, concise sentences for clarity. maximum 1-3 sentences. \n",
    "\n",
    "    # Script expected output\n",
    "\n",
    "    - **Speaker**: The character or predefined speaker delivering the lines.\n",
    "    - **Personality** The predefined personality of the voice\n",
    "    - **Accent**: The predefined accent in which the speaker should deliver their lines.\n",
    "    - **Voice**: the predefined name of the voice\n",
    "    - **Content**: The dialogue or script that the speaker will say.\n",
    "\n",
    "    Predefined Speakers:\\n{speakers_json}\n",
    "\n",
    "    # Steps\n",
    "\n",
    "    1. Parse the predefined speakers list to understand their accents.\n",
    "    2. For each story segment, assign lines to an appropriate speaker.\n",
    "    3. Ensure each line is engaging and humorous if appropriate, using short, concise language and conversational style.\n",
    "    4. Use tags such as [excitedly], [whispering], [loudly] to indicate how the lines should be said rather than writing \"spoke in blah blah in x accent\"\n",
    "    \n",
    "    # Notes\n",
    "    - Ensure the script encapsulates only enough content for a 2-minute episode. \n",
    "    - Format the JSON meticulously to avoid errors in interpretation. \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Combine messages for the chat\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": textinput},\n",
    "]\n",
    "\n",
    "params = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 5000,\n",
    "\n",
    "}\n",
    "# Structured outputs\n",
    "script = client.beta.chat.completions.parse(**params, response_format=ScriptOutput)\n",
    "\n",
    "#  Parse the JSON content\n",
    "script_data = script.choices[0].message.parsed.segments\n",
    "\n",
    "#  parse the JSON content\n",
    "script_data = script.choices[0].message.parsed.segments\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "audio_segments = []\n",
    "\n",
    "for idx, segment in enumerate(script_data):\n",
    "    accent = segment.accent\n",
    "    speaker = segment.speaker\n",
    "    voice = segment.voice\n",
    "    speech_content = segment.content\n",
    "\n",
    "    print(f\"Processing segment {idx+1}: {speaker} - {accent}\")\n",
    "\n",
    "    # completion for each segment\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini-audio-preview\",  #for better quality use gpt-4o-audio-preview-2024-12-17\n",
    "            modalities=[\"text\", \"audio\"],\n",
    "            max_tokens = 800,\n",
    "            audio={\"voice\": voice, \"format\": \"mp3\"},\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"\"\"You are a helpful assistant. output the speech provided using a {accent} accent and {personality} personality. Never say the type of accent or personality\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": speech_content\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating audio for segment {idx+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Decode audio and save to file\n",
    "    try:\n",
    "        mp3_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        speech_file_path = os.path.join(output_dir, f\"speech_{timestamp}.mp3\")\n",
    "        with open(speech_file_path, \"wb\") as f:\n",
    "            f.write(mp3_bytes)\n",
    "        print(f\"Saved audio to {speech_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving audio for segment {idx+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Load the audio segment using pydub, normalize, and store\n",
    "    try:\n",
    "        audio = AudioSegment.from_mp3(speech_file_path)\n",
    "        audio = normalize(audio)\n",
    "        audio_segments.append(audio)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio for segment {idx+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "try:\n",
    "    if os.path.exists(intro_path):\n",
    "        intro = AudioSegment.from_mp3(intro_path)\n",
    "        intro = normalize(intro)\n",
    "        print(\"Intro music added.\")\n",
    "    else:\n",
    "        intro = AudioSegment.silent(duration=500)\n",
    "        print(\"Intro music not found. Using 0.5 second of silence instead.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding intro music: {e}\")\n",
    "    intro = AudioSegment.silent(duration=1000)\n",
    "\n",
    "try:\n",
    "    if os.path.exists(outro_path):\n",
    "        outro = AudioSegment.from_mp3(outro_path)\n",
    "        outro = normalize(outro)\n",
    "        print(\"Outro music added.\")\n",
    "    else:\n",
    "        outro = AudioSegment.silent(duration=1000)\n",
    "        print(\"Outro music not found. Using 1 second of silence instead.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding outro music: {e}\")\n",
    "    outro = AudioSegment.silent(duration=1000)\n",
    "\n",
    "# Combine all audio: Intro + speech segments + Outro\n",
    "final_podcast = intro\n",
    "for audio in audio_segments:\n",
    "    final_podcast += AudioSegment.silent(duration=250)\n",
    "    final_podcast += audio\n",
    "final_podcast += outro\n",
    "\n",
    "# Export the final podcast\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "final_output_path = f\"story_{timestamp}.mp3\"\n",
    "try:\n",
    "    final_podcast.export(final_output_path, format=\"mp3\")\n",
    "    print(f\"Final output saved to {final_output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting final output: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "I hope you've had fun playing with GPT-4o's multimodal capabilities, please share what you've made with this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
